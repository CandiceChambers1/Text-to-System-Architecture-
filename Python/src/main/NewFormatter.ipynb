{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Formatter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T18:21:58.959319600Z",
     "start_time": "2023-12-13T18:21:58.946317700Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2023-12-13T18:21:59.439320300Z",
     "start_time": "2023-12-13T18:21:59.424321Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ahmii\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ahmii\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ahmii\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ahmii\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T18:22:00.027327300Z",
     "start_time": "2023-12-13T18:22:00.005326800Z"
    }
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Words to be preserved for understanding basis in Grammar\n",
    "stopwords_remove = ['from','to','and', 'it', 'with']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of Words and Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T18:22:00.884331200Z",
     "start_time": "2023-12-13T18:22:00.854331800Z"
    }
   },
   "outputs": [],
   "source": [
    "energy_list = ['Human', 'Acoustic', 'Biologcal', 'Chemical', 'Electrical', 'ElectroMagnetic', 'Hydraulic', 'Magnetic',\n",
    "               'Mechanical', 'Thermal', 'Radioactive', 'Pneumatic']\n",
    "\n",
    "# List of words present in Functional basis\n",
    "separate_list = ['separate', 'divide', 'extract', 'remove', 'isolate', 'sever', 'disjoin', 'detach', 'detaches',\n",
    "                 'release', 'sort', 'split', 'disconnect', 'subtract', 'refine', 'filter', 'purify', 'purifies', 'percolate', 'strain', 'clear', 'cut', 'drill', 'lathe', 'polish',\n",
    "                 'polishes', 'sand']\n",
    "\n",
    "distribute_list = ['distribute', 'diffuse', 'dispel', 'disperse', 'dissipate', 'diverge', 'scatter']\n",
    "\n",
    "import_list = ['import', 'form entrance', 'allow', 'allows','input', 'capture']\n",
    "\n",
    "export_list = ['export', 'dispose', 'eject', 'emit', 'empty', 'remove', 'destroy', 'eliminate']\n",
    "\n",
    "transfer_list = ['transfer', 'transport', 'transmit', 'carry', 'carries', 'deliver', 'advance', 'lift', 'move',\n",
    "                 'conduct', 'convey']\n",
    "guide_list = ['guide', 'translate', 'rotate','direct', 'shift', 'steer', 'straighten', 'switch', 'switches',\n",
    "              'move', 'relocate', 'spin', 'turn', 'constrain',\n",
    "              'unfasten', 'unlock']\n",
    "couple_list = ['couple', 'join', 'link', 'associate', 'assemble', 'fasten', 'attach', 'attaches']\n",
    "\n",
    "\n",
    "\n",
    "mix_list = ['mix', 'mixes', 'add', 'blend', 'coalesce', 'combine', 'pack']\n",
    "\n",
    "actuate_list = ['actuate', 'enable', 'initiate', 'start', 'turn on']\n",
    "\n",
    "regulate_list = ['regulate', 'increase', 'decrease', 'control', 'equalize', 'limit', 'maintain', 'open', 'close',\n",
    "                 'delay', 'interrupt']\n",
    "change_list = ['change', 'increment', 'decrement', 'shape', 'condition', 'adjust', 'modulate', 'demodulate', 'invert',\n",
    "               'normalize', 'rectify',\n",
    "               'rectifies', 'reset', 'scale', 'vary', 'varies', 'modify', 'modifies', 'amplify', 'amplifies', 'enhance',\n",
    "               'magnify', 'magnifies',\n",
    "               'multiply', 'multiplies', 'attenuate', 'dampen', 'reduce', 'compact', 'compress', 'compresses', 'crush',\n",
    "               'crushes', 'pierce', 'deform', 'form', 'prepare', 'adapt', 'treat']\n",
    "\n",
    "stop_list = ['stop', 'prevent', 'inhibit', 'end', 'hault', 'pause', 'interrupt', 'restrain', 'disable', 'turn off',\n",
    "             'shield', 'insulate', 'protect', 'resist']\n",
    "\n",
    "convert_list = ['convert', 'condense', 'create', 'decode', 'differentiate', 'digitize', 'encode', 'evaporate',\n",
    "                'generate', 'integrate', 'liquefy', 'liquifies', 'process', 'solidify',\n",
    "                'solidifies', 'transform']\n",
    "store_list = ['store', 'contain', 'collect', 'accumulate', 'enclose', 'absorb', 'consume', 'fill', 'reserve']\n",
    "\n",
    "supply_list = ['supply', 'supplies', 'provide', 'replenish', 'replenishes', 'retrieve']\n",
    "\n",
    "sense_list = ['sense', 'detect', 'measure', 'feel', 'determine', 'discern', 'perceive', 'recognize', 'identify',\n",
    "              'identifies', 'locate']\n",
    "indicate_list = ['indicate', 'track', 'display', 'announce', 'show', 'denote', 'record', 'register', 'mark', 'time',\n",
    "                 'expose', 'select']\n",
    "process_list = ['process', 'processes', 'calculate', 'check']\n",
    "\n",
    "energize_list = ['energize', 'deenergize']\n",
    "\n",
    "# Different adjectives to be used as a Fuzzy system\n",
    "temperature = ['Cryogenic', 'Frozen', 'Chilled', 'Freezing', 'Cold', 'Cool', 'Nomral Temperature', 'Room Temperature',\n",
    "               'Lukewarm', 'Toasty', 'Mild', 'Warm',\n",
    "               'Heated', 'Hot', 'Cooked', 'Toasted', 'Boiling', 'Burning', 'Steaming']\n",
    "velocity = ['Static', 'Still', 'Creeping', 'Sluggish', 'Slow', 'Flowing', 'Moving', 'Fast', 'Rapid']\n",
    "material = ['Solid', 'Liquid', 'Gas', 'Mixture']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T18:22:01.256333200Z",
     "start_time": "2023-12-13T18:22:01.243332900Z"
    }
   },
   "outputs": [],
   "source": [
    "functional_verbs = []\n",
    "all_lists = [separate_list, distribute_list, import_list, export_list, \n",
    "            transfer_list, guide_list, couple_list, mix_list, actuate_list, \n",
    "            regulate_list, change_list, stop_list, convert_list, store_list, \n",
    "            supply_list, sense_list, indicate_list, process_list, \n",
    "            energize_list]\n",
    "\n",
    "allPreserveList = [temperature, functional_verbs, velocity, stopwords_remove, \n",
    "                  energy_list, material]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Characters in Front of the Reserved Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T18:22:01.740336800Z",
     "start_time": "2023-12-13T18:22:01.728338400Z"
    }
   },
   "outputs": [],
   "source": [
    "def addCharacters(List):\n",
    "    new_list = []\n",
    "    for elem in List:\n",
    "        if elem[-1] == \"h\":\n",
    "            new_list.append(elem + 'es')\n",
    "        elif elem[-1] == 'y':\n",
    "            if elem[-2] in ['a', 'e', 'i', 'o', 'u']:\n",
    "                new_list.append(elem + 's')\n",
    "            else:\n",
    "                new_list.append(elem[:-1] + 'ies')\n",
    "        elif elem[-1] == 's':\n",
    "            if elem[-2] == 's':\n",
    "                new_list.append(elem + 'es')\n",
    "            else:\n",
    "                new_list.append(elem + 's')\n",
    "        elif elem[-1] == 'x':\n",
    "            new_list.append(elem + 'es')\n",
    "        else:\n",
    "            new_list.append(elem + 's')\n",
    "    List = []\n",
    "    List.extend(new_list)\n",
    "    \n",
    "    return List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T18:22:01.996338800Z",
     "start_time": "2023-12-13T18:22:01.966339Z"
    }
   },
   "outputs": [],
   "source": [
    "def editLists(all_lists):\n",
    "    temporary_list = []\n",
    "    new_all_lists = []\n",
    "    for i in all_lists: \n",
    "        extended_list = addCharacters(i)\n",
    "        temporary_list = extended_list\n",
    "        new_all_lists.append(temporary_list)\n",
    "        temporary_list = []\n",
    "        for i in extended_list:\n",
    "            functional_verbs.append(i)\n",
    "            i = functional_verbs\n",
    "            \n",
    "    return new_all_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perserving words present in temperature, velocity, energy and functional verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T18:22:02.380342800Z",
     "start_time": "2023-12-13T18:22:02.368341900Z"
    }
   },
   "outputs": [],
   "source": [
    "def perserveWords(allPreserveList):\n",
    "    for j in allPerserveList: \n",
    "        if i.lower() in stop_words:\n",
    "            stop_words.remove(i.lower())\n",
    "\n",
    "#stop_words.add('The','A')\n",
    "# Adding some words to the stop words list that we do not require and removing words we need \n",
    "stopwords_add = ['The', 'A']\n",
    "for i in stopwords_add:\n",
    "    stop_words.add(i)\n",
    "\n",
    "for i in stopwords_remove:\n",
    "    stop_words.remove(i)\n",
    "    \n",
    "ps = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T18:22:02.807357400Z",
     "start_time": "2023-12-13T18:22:02.789345800Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_stopwords_from_raw_text(input_paragraph):\n",
    "    try:\n",
    "        words = nltk.word_tokenize(input_paragraph)\n",
    "        return remove_stopwords_from_tokenized_text(words)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "def remove_stopwords_from_tokenized_text(words):\n",
    "    try:\n",
    "        output_list = []\n",
    "\n",
    "        for w in words:\n",
    "            if w not in stop_words:\n",
    "                output_list.append(w)\n",
    "\n",
    "        return output_list\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T18:22:03.046347200Z",
     "start_time": "2023-12-13T18:22:03.032346200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Consists can be used in the same way as before\n",
    "# Converting words not in nltk synonym list to the ones that are accepted\n",
    "def changeSynonyms(final_list, new_all_list):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    index = 0\n",
    "    one_list = []\n",
    "    \n",
    "    for i in final_list:\n",
    "        for j in new_all_list:\n",
    "            one_list = j\n",
    "            for k in one_list:\n",
    "                if (i == k):\n",
    "                    if i.lower()[-1] == 's':\n",
    "                        if one_list[0][-1] == 'x':\n",
    "                            final_list[index] = one_list[0] + 'es'\n",
    "                        elif one_list[0][-1] == 'y':\n",
    "                            final_list[index] = one_list[0][:-1] + 'ies'\n",
    "                        else:\n",
    "                            final_list[index] = one_list[0]\n",
    "                    break       \n",
    "        index = index + 1\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T18:22:03.248347900Z",
     "start_time": "2023-12-13T18:22:03.242346800Z"
    }
   },
   "outputs": [],
   "source": [
    "def createParagraphs(final_list): \n",
    "    new_para = \"\"\n",
    "    for ele in final_list:\n",
    "        if ele == \".\":\n",
    "            new_para = new_para + ele\n",
    "        else:\n",
    "            new_para = new_para + \" \" + ele\n",
    "    return new_para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T18:22:03.484349100Z",
     "start_time": "2023-12-13T18:22:03.457348900Z"
    }
   },
   "outputs": [],
   "source": [
    "def formatNouns(new_para):\n",
    "    new_para_latest = \"\"\n",
    "    for i in new_para.split(\".\"):\n",
    "        if \"consists\" in i:\n",
    "            within_words = i.split()\n",
    "            for words in within_words:\n",
    "                if (words == \"and\" or words == \"consists\" or words == \",\"):\n",
    "                    new_para_latest = new_para_latest[:-1] + \" \" + words + \" \"\n",
    "                else:\n",
    "                    new_para_latest = new_para_latest + words.upper() + \"_\"\n",
    "            new_para_latest = new_para_latest[:-1] + \".\\n\"\n",
    "        elif \"connected\" in i:\n",
    "            within_words = i.split()\n",
    "            for words in within_words:\n",
    "                if (words == \"connected\" or words == \"and\" or words == ','):\n",
    "                    new_para_latest = new_para_latest[:-1] + \" \" + words + \" \"\n",
    "                elif (words == \"to\"):\n",
    "                    new_para_latest = new_para_latest + \" \" + words + \" \"\n",
    "                else:\n",
    "                    new_para_latest = new_para_latest + words.upper() + \"_\"\n",
    "            new_para_latest = new_para_latest[:-1] + \".\\n\"\n",
    "        elif (\"imports\" in i) or (\"exports\" in i) or (\"transfers\" in i) or (\"guides\" in i) or (\"supplies\" in i):\n",
    "            within_words = i.split()\n",
    "            index_func = 0\n",
    "            index_from = 0\n",
    "            index_to = 0            \n",
    "            for m in range(len(within_words)):\n",
    "                words = ['imports', 'exports', 'transfers', 'guides', 'supplies']\n",
    "                for i in words:\n",
    "                    if i in within_words:\n",
    "                        occured_word = i\n",
    "                index_func = within_words.index(occured_word)\n",
    "                if (within_words[m] == \"from\"):\n",
    "                    index_from = m\n",
    "                elif (within_words[m] == \"to\"):\n",
    "                    index_to = m\n",
    "                else:\n",
    "                    continue\n",
    "            m = 0\n",
    "            while (m < len(within_words)):\n",
    "                if (index_func > 0 and m == 0):\n",
    "                    for x in range(index_func):\n",
    "                        new_para_latest = new_para_latest + within_words[m].upper() + \"_\"\n",
    "                        m = m + 1\n",
    "                    new_para_latest = new_para_latest[:-1]\n",
    "                elif (index_from > 0 and index_to > 0 and m == index_from):\n",
    "                    new_para_latest = new_para_latest + \" from\" + \" \"\n",
    "                    m = m + 1\n",
    "                    for x in range(index_from + 1, index_to):\n",
    "                        new_para_latest = new_para_latest + within_words[m].upper() + \"_\"\n",
    "                        m = m + 1\n",
    "                    new_para_latest = new_para_latest[:-1] + \" to \"\n",
    "                    m = m + 1\n",
    "                    for x in range(index_to + 1, len(within_words)):\n",
    "                        new_para_latest = new_para_latest + within_words[m].upper() + \"_\"\n",
    "                        m = m + 1\n",
    "                    new_para_latest = new_para_latest[:-1]\n",
    "                                        \n",
    "                elif (index_from > 0 and m == index_from):\n",
    "                    new_para_latest = new_para_latest + \" from \"\n",
    "                    m = m + 1\n",
    "                    for x in range(index_from + 1, len(within_words)):\n",
    "                        new_para_latest = new_para_latest + within_words[m].upper() + \"_\"\n",
    "                        m = m + 1\n",
    "                    new_para_latest = new_para_latest[:-1]\n",
    "                elif (index_to > 0 and m == index_to):\n",
    "                    new_para_latest = new_para_latest + \" to \"\n",
    "                    m = m + 1\n",
    "                    for x in range(index_to + 1, len(within_words)):\n",
    "                        new_para_latest = new_para_latest + within_words[m].upper() + \"_\"\n",
    "                        m = m + 1\n",
    "                    new_para_latest = new_para_latest[:-1]\n",
    "                else:\n",
    "                    new_para_latest = new_para_latest + \" \" + within_words[m]\n",
    "                    m += 1\n",
    "            new_para_latest = new_para_latest + \".\\n\"\n",
    "        \n",
    "        elif ((\"converts\" in i) or (\"mixes\" in i) or (\"couples\" in i) or (\"separates\" in i) or (\"energizes\" in i) or (\n",
    "                \"deenergizes\" in i) or (\"stores\" in i) or (\"stops\" in i) or (\"changes\" in i) or (\"regulates\" in i)):\n",
    "            within_words = i.split()\n",
    "            words = ['converts', 'mixes', 'couples', 'separates', 'energizes', 'deenergizes', 'stores', 'stops', 'changes',\n",
    "                     'regulates']\n",
    "            for i in words:\n",
    "                if i in within_words:\n",
    "                    occured_word = i\n",
    "            index_of_ow = within_words.index(occured_word)\n",
    "            for j in range(len(within_words)):\n",
    "                if j < index_of_ow:\n",
    "                    new_para_latest = new_para_latest + within_words[j].upper() + \"_\"\n",
    "                elif j == index_of_ow:\n",
    "                    new_para_latest = new_para_latest[:-1] + \" \" + within_words[j]\n",
    "                else:\n",
    "                    new_para_latest = new_para_latest + \" \" + within_words[j]\n",
    "            new_para_latest = new_para_latest + \".\\n\"\n",
    "        elif \"distributes\" in i:\n",
    "            within_words = i.split()\n",
    "            index = within_words.index(\"distributes\")\n",
    "            index_to = within_words.index(\"to\")\n",
    "            for x in range(len(within_words)):\n",
    "                if x < index:\n",
    "                    new_para_latest = new_para_latest + within_words[x].upper() + \"_\"\n",
    "                elif x == index:\n",
    "                    new_para_latest = new_para_latest[:-1] + \" \" + within_words[x]\n",
    "                elif x == index_to:\n",
    "                    new_para_latest = new_para_latest + \" \" + within_words[x] + \" \"\n",
    "                else:\n",
    "                    if x > index_to:\n",
    "                        if within_words[x] == \"and\":\n",
    "                            new_para_latest = new_para_latest[:-1] + \" \" + within_words[x] + \" \"\n",
    "                        else:\n",
    "                            new_para_latest = new_para_latest + within_words[x].upper() + \"_\"\n",
    "                    else:\n",
    "                        new_para_latest = new_para_latest + \" \" + within_words[x]\n",
    "            new_para_latest = new_para_latest[:-1] + \".\\n\"\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return new_para_latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T18:22:03.711350400Z",
     "start_time": "2023-12-13T18:22:03.694352400Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_synonyms(word):\n",
    "    var = False\n",
    "    exchange_word = ''\n",
    "    try:\n",
    "        stemmed_word = ps.stem(word)\n",
    "        if ((word == '.') or (word == ',')):\n",
    "            return word\n",
    "        new_word = '\"' + stemmed_word + '\"'\n",
    "        for syn in wordnet.synsets(new_word):\n",
    "            for lm in syn.lemmas():\n",
    "                for x in functional_verbs:\n",
    "                    if (lm.name()).lower() == x.lower():\n",
    "                        var = True;\n",
    "                        exchange_word = x\n",
    "        if var == True:\n",
    "            return exchange_word\n",
    "            var = False\n",
    "        else:\n",
    "            return word\n",
    "    except Exception as e:\n",
    "        word.upper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T18:22:03.910353900Z",
     "start_time": "2023-12-13T18:22:03.899354Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_if_in_NLTK(new_para_latest):\n",
    "    new_final_list = []\n",
    "    for item in word_tokenize(new_para_latest):\n",
    "        var = False\n",
    "        for i in energy_list:\n",
    "            if i.lower() == item.lower():\n",
    "                new_final_list.append(i)\n",
    "                var = True\n",
    "                break\n",
    "        if var == True:\n",
    "            continue\n",
    "        else:\n",
    "            for j in temperature:\n",
    "                if j.lower() == item.lower():\n",
    "                    new_final_list.append(j)\n",
    "                    var = True\n",
    "                    break\n",
    "            if var:\n",
    "                continue\n",
    "            else:\n",
    "                for k in velocity:\n",
    "                    if k.lower() == item.lower():\n",
    "                        new_final_list.append(k)\n",
    "                        var = True\n",
    "                        break\n",
    "                if var:\n",
    "                    continue\n",
    "                else:\n",
    "                    for l in material:\n",
    "                        if l.lower() == item.lower():\n",
    "                            new_final_list.append(l)\n",
    "                            var = True\n",
    "                            break\n",
    "                    if var:\n",
    "                        continue\n",
    "                    else:\n",
    "                        new_item = convert_synonyms(item)\n",
    "                        if (new_item == None):\n",
    "                            new_final_list.append(item)\n",
    "                        else:\n",
    "                            new_final_list.append(new_item)\n",
    "\n",
    "                            \n",
    "    return new_final_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T18:22:04.126353600Z",
     "start_time": "2023-12-13T18:22:04.099353700Z"
    }
   },
   "outputs": [],
   "source": [
    "def finalVersion(new_final_list):\n",
    "    str = ''\n",
    "    for element in new_final_list:\n",
    "        if element == '.':\n",
    "            str = str + element + \"\\n\"\n",
    "        else:\n",
    "            str = str + \" \" + element\n",
    "\n",
    "    send_to_grammar = \"\"\n",
    "    additional_reserved_words = [',', 'consists', 'connected']\n",
    "    for i in str.split(\".\"):\n",
    "        indexes = []\n",
    "        within_words = i.split()\n",
    "        for j in within_words:\n",
    "            if (j.isupper() or (j in functional_verbs) or (j in temperature) or (j in velocity) or (j in material) or (\n",
    "                    j in additional_reserved_words) or (j in stopwords_remove)):\n",
    "                continue\n",
    "            else:\n",
    "                indexes.append(within_words.index(j))\n",
    "        if len(indexes) >= 2:\n",
    "            for j in range(len(indexes) - 1):\n",
    "                if (indexes[j] + 1 == indexes[j + 1]):\n",
    "                    within_words[indexes[j]] = within_words[indexes[j]][:1].upper() + within_words[indexes[j]][1:]\n",
    "        if (i != \"\\n\"):\n",
    "            for j in within_words:\n",
    "                send_to_grammar = send_to_grammar + j + \" \"\n",
    "            send_to_grammar = send_to_grammar[:-1] + \".\\n\"\n",
    "\n",
    "    return send_to_grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T18:22:04.919359Z",
     "start_time": "2023-12-13T18:22:04.905361Z"
    }
   },
   "outputs": [],
   "source": [
    "# input_paragraph = \"The FGS System consists of the Left Side FGS, the Right Side FGS, a LR Bus, and a RL Bus. LR Bus is connected to Left Side FGS and Right Side FGS. RL Bus is connected to Left Side FGS and Right Side FGS. The Left Side FGS imports boolean Input from the Left Transfer Switch. The Left Side FGS imports boolean Input from the Left Primary Side. The Right Side FGS imports boolean Input from the Right Transfer Switch. The Right Side FGS imports boolean Input from the Right Primary Side. CLK_ONE supplies synchronous value to Left Side FGS. CLK_TWO supplies synchronous value to LR_Bus. CLK_THREE supplies synchronous value to Right Side FGS. CLK_FOUR supplies synchronous value to RL_Bus.\"\n",
    "# input_paragraph=\"The Flight Control System consists of a Flight Guidance System, Side, and Bus. The Flight Guidance System consists of internal components: Left Flight Guidance System, Right Flight Guidance System, Left Right Bus, and Right Left Bus. The Side consists of ports Transfer Switch, Primary Side, Pilot Flying, Bus-In and Clock. The Bus consists of ports Left, Right, and Clock. The Left Flight Guidance System instantiates a Side. Right Flight Guidance System instantiates a Side. The Left Right instantiates a Bus. The Right Left instantiates a Bus. The Pilot Flying exports a boolean value. The Bus-In imports a boolean input. The Left Flight Guidance System is connected to Left Right by the ports Pilot Flying and Left. The Right Left is connected to Left Flight Guidance System by the ports Left and Bus-In. The Left Right is connected to Right Flight Guidance System by the ports Right and Bus-In. The Right Flight Guidance System is connected to Right Left by the ports Pilot Flying and Right.\"\n",
    "# input_paragraph = \"The ASS consists of an Active Standby System, a Side and a Bus. The Active Standby System consists of internal components: Side1, Side2, Bus12 and Bus21. The Side consists of ports Manual Selection, Side1SubSystemStatus, Side2SubSystemStatus, CLK, PrimarySide, Status and Failed. The Bus consists of ports In, Out and CLK. Side1 instantiates a Side. Side2 instantiates a Side. Bus12 instantiates a Bus. Bus21 instantiates a Bus. Status exports an output. Side1 is connected to Bus12 by the ports Status and In. Bus12 is connected to Side2 by the ports Out and Side1SubSystemsStatus. Side2 is connected to Bus21 by the ports Status and In. Bus21 is connected to Side1 by the ports Out and Side2SubSystemStatus.\"\n",
    "# input_paragraph = \"The coffeemaker consists of a cooking unit and a pot. The pot is connected to the cooking unit. The cooking unit consists of a tank, a heating unit, and a brewing unit. The tank is connected to the heating unit. The heating unit is connected to the brewing unit. The pot consists of a glassware, a lid, and a handle. The lid is connected to the glassware. The handle is connected to the glassware. The tank imports water and transfers it to the heating unit. The heating unit consists of a heating coil and a hot water pipe. The heating coil is connected to the hot water pipe. The heating coil imports electricity and converts it to heat. The heating coil transfers heat to the hot water pipe. The hot water pipe receives water from the tank. The hot water pipe imports heat from the heating coil. The hot water pipe imports water and heat and energizes it to hot water. The brewing unit consists of a vertical pipe, a water valve, a shower head, a filter, and a filter holder. The lower end of the vertical pipe is connected to the hot water pipe. The water valve is connected to the vertical pipe. The shower head is connected to the upper end of the vertical pipe. The filer holder consists of a filter. The vertical pipe receives hot water from the hot water pipe. The vertical pipe transfers hot water to the shower head. The shower head receives hot water from the vertical pipe. The shower head distributes it to filter. The filter imports ground coffee. The filter receives hot water from the shower head. The filter couples ground coffee and hot water. The pot stores the liquid coffee.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Driver Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T18:22:06.034368200Z",
     "start_time": "2023-12-13T18:22:06.027368400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FGS_SYSTEM consists LEFT_SIDE_FGS , RIGHT_SIDE_FGS , LR_BUS , and RL_BUS.\n",
      "LR_BUS connected to LEFT_SIDE_FGS and RIGHT_SIDE_FGS.\n",
      "RL_BUS connected to LEFT_SIDE_FGS and RIGHT_SIDE_FGS.\n",
      "LEFT_SIDE_FGS imports Boolean Input from LEFT_TRANSFER_SWITCH.\n",
      "LEFT_SIDE_FGS imports Boolean Input from LEFT_PRIMARY_SIDE.\n",
      "RIGHT_SIDE_FGS imports Boolean Input from RIGHT_TRANSFER_SWITCH.\n",
      "RIGHT_SIDE_FGS imports Boolean Input from RIGHT_PRIMARY_SIDE.\n",
      "CLK_ONE supplies Synchronous value to LEFT_SIDE_FGS.\n",
      "CLK_TWO supplies Synchronous value to LR_BUS.\n",
      "CLK_THREE supplies Synchronous value to RIGHT_SIDE_FGS.\n",
      "CLK_FOUR supplies Synchronous value to RL_BUS.\n"
     ]
    }
   ],
   "source": [
    "# Driver Code\n",
    "if __name__ == \"__main__\":\n",
    "    functional_verbs = []\n",
    "    final_list = []\n",
    "    new_para = ''\n",
    "    new_para_latest = ''\n",
    "    new_final_list = []\n",
    "    send_to_grammar = ''\n",
    "    new_all_list = []\n",
    "    \n",
    "    # Edit all individual list and update the all_lists\n",
    "    new_all_list = editLists(all_lists)\n",
    "    \n",
    "    # Remove stopwords from raw text \n",
    "    final_list = remove_stopwords_from_raw_text(input_paragraph)\n",
    "    \n",
    "    # Change synonyms to our preferred  words\n",
    "    changeSynonyms(final_list, new_all_list)\n",
    "    \n",
    "    # Convert the list to a paragraph \n",
    "    new_para = createParagraphs(final_list)\n",
    "    \n",
    "    # Reformat the nouns for ANTLR standards \n",
    "    new_para_latest = formatNouns(new_para)\n",
    "    \n",
    "    # \n",
    "    new_final_list = convert_if_in_NLTK(new_para_latest)\n",
    "    send_to_grammar = finalVersion(new_final_list)\n",
    "    \n",
    "    print(send_to_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T18:16:33.775482800Z",
     "start_time": "2023-12-13T18:16:33.714481600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T18:16:33.792483900Z",
     "start_time": "2023-12-13T18:16:33.731484500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T18:16:33.795483600Z",
     "start_time": "2023-12-13T18:16:33.747482300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T18:16:33.797482300Z",
     "start_time": "2023-12-13T18:16:33.763485800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T18:16:33.813482700Z",
     "start_time": "2023-12-13T18:16:33.778482800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T18:16:33.845483Z",
     "start_time": "2023-12-13T18:16:33.794483300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T18:16:33.848482500Z",
     "start_time": "2023-12-13T18:16:33.810484900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T18:16:33.848482500Z",
     "start_time": "2023-12-13T18:16:33.825485800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T18:16:33.867483600Z",
     "start_time": "2023-12-13T18:16:33.841483Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T18:16:33.896485800Z",
     "start_time": "2023-12-13T18:16:33.860483200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T18:16:33.898485500Z",
     "start_time": "2023-12-13T18:16:33.875483500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T18:16:33.914483400Z",
     "start_time": "2023-12-13T18:16:33.890483200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T18:16:33.954514400Z",
     "start_time": "2023-12-13T18:16:33.906483Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T18:16:33.956483600Z",
     "start_time": "2023-12-13T18:16:33.921485100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T18:16:33.956483600Z",
     "start_time": "2023-12-13T18:16:33.937483800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T18:16:33.975484Z",
     "start_time": "2023-12-13T18:16:33.953485600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T18:16:34.002486600Z",
     "start_time": "2023-12-13T18:16:33.967485100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
